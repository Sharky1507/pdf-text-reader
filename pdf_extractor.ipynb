{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed470fa0",
   "metadata": {},
   "source": [
    "# PDF Document Extraction Tool\n",
    "\n",
    "This notebook provides functionality to extract text and tables from PDF documents and format them into a structured JSON file.\n",
    "\n",
    "**User Story:** As a user, I should provide a path of a PDF, and the program should display the text from the PDF.\n",
    "\n",
    "**Features:**\n",
    "- Extract regular text from PDFs\n",
    "- Extract tabular data from PDFs\n",
    "- Format extracted data into JSON\n",
    "- Save the JSON output to a file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493b5fae",
   "metadata": {},
   "source": [
    "## Install Required Libraries\n",
    "\n",
    "First, let's install the necessary libraries for PDF processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab5507c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting tabula-py\n",
      "  Downloading tabula_py-2.10.0-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Collecting camelot-py\n",
      "  Downloading camelot_py-1.0.0-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting opencv-python-headless\n",
      "  Downloading opencv_python_headless-4.11.0.86-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Collecting numpy>1.24.4 (from tabula-py)\n",
      "  Downloading numpy-2.2.5-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Collecting distro (from tabula-py)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\kafee\\downloads\\document_extraction\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting click>=8.0.1 (from camelot-py)\n",
      "  Downloading click-8.2.0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting chardet>=5.1.0 (from camelot-py)\n",
      "  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting openpyxl>=3.1.0 (from camelot-py)\n",
      "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting pdfminer-six>=20240706 (from camelot-py)\n",
      "  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting pypdf<6.0,>=4.0 (from camelot-py)\n",
      "  Downloading pypdf-5.5.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting tabulate>=0.9.0 (from camelot-py)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting pypdfium2>=4 (from camelot-py)\n",
      "  Downloading pypdfium2-4.30.1-py3-none-win_amd64.whl.metadata (48 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\kafee\\downloads\\document_extraction\\.venv\\lib\\site-packages (from click>=8.0.1->camelot-py) (0.4.6)\n",
      "Collecting et-xmlfile (from openpyxl>=3.1.0->camelot-py)\n",
      "  Downloading et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting charset-normalizer>=2.0.0 (from pdfminer-six>=20240706->camelot-py)\n",
      "  Downloading charset_normalizer-3.4.2-cp312-cp312-win_amd64.whl.metadata (36 kB)\n",
      "Collecting cryptography>=36.0.0 (from pdfminer-six>=20240706->camelot-py)\n",
      "  Downloading cryptography-44.0.3-cp39-abi3-win_amd64.whl.metadata (5.7 kB)\n",
      "Collecting cffi>=1.12 (from cryptography>=36.0.0->pdfminer-six>=20240706->camelot-py)\n",
      "  Downloading cffi-1.17.1-cp312-cp312-win_amd64.whl.metadata (1.6 kB)\n",
      "Collecting pycparser (from cffi>=1.12->cryptography>=36.0.0->pdfminer-six>=20240706->camelot-py)\n",
      "  Downloading pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kafee\\downloads\\document_extraction\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "Downloading tabula_py-2.10.0-py3-none-any.whl (12.0 MB)\n",
      "   ---------------------------------------- 0.0/12.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 3.1/12.0 MB 18.5 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 4.7/12.0 MB 11.4 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.5/12.0 MB 9.6 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 6.3/12.0 MB 7.3 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 6.8/12.0 MB 6.5 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 7.3/12.0 MB 6.0 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 8.1/12.0 MB 5.5 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 9.2/12.0 MB 5.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.7/12.0 MB 5.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.8/12.0 MB 5.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.0/12.0 MB 5.3 MB/s eta 0:00:00\n",
      "Downloading pandas-2.2.3-cp312-cp312-win_amd64.whl (11.5 MB)\n",
      "   ---------------------------------------- 0.0/11.5 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 1.0/11.5 MB 6.3 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 2.6/11.5 MB 6.9 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 3.9/11.5 MB 6.7 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 4.5/11.5 MB 6.1 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 5.2/11.5 MB 5.2 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 6.3/11.5 MB 5.1 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 7.3/11.5 MB 5.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 8.7/11.5 MB 5.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 9.7/11.5 MB 5.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.5/11.5 MB 5.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.0/11.5 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.5/11.5 MB 4.6 MB/s eta 0:00:00\n",
      "Downloading camelot_py-1.0.0-py3-none-any.whl (66 kB)\n",
      "Downloading pypdf-5.5.0-py3-none-any.whl (303 kB)\n",
      "Downloading opencv_python_headless-4.11.0.86-cp37-abi3-win_amd64.whl (39.4 MB)\n",
      "   ---------------------------------------- 0.0/39.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.0/39.4 MB 6.3 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 2.6/39.4 MB 6.9 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 4.7/39.4 MB 7.7 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 5.8/39.4 MB 7.5 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 6.3/39.4 MB 6.7 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 6.8/39.4 MB 5.7 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 7.9/39.4 MB 5.3 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 8.9/39.4 MB 5.4 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 10.5/39.4 MB 5.5 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 12.1/39.4 MB 5.8 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 13.4/39.4 MB 5.8 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 14.2/39.4 MB 5.7 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 14.7/39.4 MB 5.4 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 15.5/39.4 MB 5.3 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 16.8/39.4 MB 5.3 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 18.1/39.4 MB 5.3 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 18.9/39.4 MB 5.3 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 19.9/39.4 MB 5.2 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 21.2/39.4 MB 5.3 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 22.3/39.4 MB 5.3 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 23.3/39.4 MB 5.3 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 24.1/39.4 MB 5.2 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 25.2/39.4 MB 5.2 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 26.2/39.4 MB 5.2 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 27.0/39.4 MB 5.1 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 28.0/39.4 MB 5.1 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 29.4/39.4 MB 5.1 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 30.4/39.4 MB 5.2 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 31.5/39.4 MB 5.1 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 32.0/39.4 MB 5.1 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 32.5/39.4 MB 5.0 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 33.3/39.4 MB 5.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 34.3/39.4 MB 4.9 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 35.9/39.4 MB 5.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 37.7/39.4 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.5/39.4 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  39.1/39.4 MB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 39.4/39.4 MB 5.0 MB/s eta 0:00:00\n",
      "Downloading chardet-5.2.0-py3-none-any.whl (199 kB)\n",
      "Downloading click-8.2.0-py3-none-any.whl (102 kB)\n",
      "Downloading numpy-2.2.5-cp312-cp312-win_amd64.whl (12.6 MB)\n",
      "   ---------------------------------------- 0.0/12.6 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 1.0/12.6 MB 5.6 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 2.4/12.6 MB 6.1 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.9/12.6 MB 6.7 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 5.0/12.6 MB 6.4 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 6.3/12.6 MB 6.2 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 6.8/12.6 MB 5.7 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 7.9/12.6 MB 5.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 9.2/12.6 MB 5.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 10.0/12.6 MB 5.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.8/12.6 MB 5.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.6/12.6 MB 5.7 MB/s eta 0:00:00\n",
      "Downloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "Downloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n",
      "   ---------------------------------------- 0.0/5.6 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.5/5.6 MB 3.3 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 1.3/5.6 MB 3.9 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 2.1/5.6 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 3.4/5.6 MB 4.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 4.7/5.6 MB 4.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.6/5.6 MB 5.0 MB/s eta 0:00:00\n",
      "Downloading charset_normalizer-3.4.2-cp312-cp312-win_amd64.whl (105 kB)\n",
      "Downloading cryptography-44.0.3-cp39-abi3-win_amd64.whl (3.2 MB)\n",
      "   ---------------------------------------- 0.0/3.2 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.5/3.2 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 1.3/3.2 MB 3.1 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 2.1/3.2 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.2/3.2 MB 3.9 MB/s eta 0:00:00\n",
      "Downloading cffi-1.17.1-cp312-cp312-win_amd64.whl (181 kB)\n",
      "Downloading pypdfium2-4.30.1-py3-none-win_amd64.whl (3.0 MB)\n",
      "   ---------------------------------------- 0.0/3.0 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 1.0/3.0 MB 6.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 2.6/3.0 MB 6.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.0/3.0 MB 6.0 MB/s eta 0:00:00\n",
      "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Downloading pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Installing collected packages: pytz, tzdata, tabulate, pypdfium2, PyPDF2, pypdf, pycparser, numpy, et-xmlfile, distro, click, charset-normalizer, chardet, pandas, openpyxl, opencv-python-headless, cffi, tabula-py, cryptography, pdfminer-six, camelot-py\n",
      "\n",
      "   ----------------------------------------  0/21 [pytz]\n",
      "   ----------------------------------------  0/21 [pytz]\n",
      "   - --------------------------------------  1/21 [tzdata]\n",
      "   - --------------------------------------  1/21 [tzdata]\n",
      "   --- ------------------------------------  2/21 [tabulate]\n",
      "   ----- ----------------------------------  3/21 [pypdfium2]\n",
      "   ----- ----------------------------------  3/21 [pypdfium2]\n",
      "   ----- ----------------------------------  3/21 [pypdfium2]\n",
      "   ----- ----------------------------------  3/21 [pypdfium2]\n",
      "   ------- --------------------------------  4/21 [PyPDF2]\n",
      "   ------- --------------------------------  4/21 [PyPDF2]\n",
      "   ------- --------------------------------  4/21 [PyPDF2]\n",
      "   ------- --------------------------------  4/21 [PyPDF2]\n",
      "   ------- --------------------------------  4/21 [PyPDF2]\n",
      "   ------- --------------------------------  4/21 [PyPDF2]\n",
      "   --------- ------------------------------  5/21 [pypdf]\n",
      "   --------- ------------------------------  5/21 [pypdf]\n",
      "   --------- ------------------------------  5/21 [pypdf]\n",
      "   --------- ------------------------------  5/21 [pypdf]\n",
      "   --------- ------------------------------  5/21 [pypdf]\n",
      "   --------- ------------------------------  5/21 [pypdf]\n",
      "   ----------- ----------------------------  6/21 [pycparser]\n",
      "   ----------- ----------------------------  6/21 [pycparser]\n",
      "   ----------- ----------------------------  6/21 [pycparser]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ------------- --------------------------  7/21 [numpy]\n",
      "   ----------------- ----------------------  9/21 [distro]\n",
      "   ------------------- -------------------- 10/21 [click]\n",
      "   ------------------- -------------------- 10/21 [click]\n",
      "   -------------------- ------------------- 11/21 [charset-normalizer]\n",
      "   -------------------- ------------------- 11/21 [charset-normalizer]\n",
      "   ---------------------- ----------------- 12/21 [chardet]\n",
      "   ---------------------- ----------------- 12/21 [chardet]\n",
      "   ---------------------- ----------------- 12/21 [chardet]\n",
      "   ---------------------- ----------------- 12/21 [chardet]\n",
      "   ---------------------- ----------------- 12/21 [chardet]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   ------------------------ --------------- 13/21 [pandas]\n",
      "   -------------------------- ------------- 14/21 [openpyxl]\n",
      "   -------------------------- ------------- 14/21 [openpyxl]\n",
      "   -------------------------- ------------- 14/21 [openpyxl]\n",
      "   -------------------------- ------------- 14/21 [openpyxl]\n",
      "   -------------------------- ------------- 14/21 [openpyxl]\n",
      "   -------------------------- ------------- 14/21 [openpyxl]\n",
      "   -------------------------- ------------- 14/21 [openpyxl]\n",
      "   -------------------------- ------------- 14/21 [openpyxl]\n",
      "   -------------------------- ------------- 14/21 [openpyxl]\n",
      "   -------------------------- ------------- 14/21 [openpyxl]\n",
      "   -------------------------- ------------- 14/21 [openpyxl]\n",
      "   -------------------------- ------------- 14/21 [openpyxl]\n",
      "   -------------------------- ------------- 14/21 [openpyxl]\n",
      "   -------------------------- ------------- 14/21 [openpyxl]\n",
      "   -------------------------- ------------- 14/21 [openpyxl]\n",
      "   -------------------------- ------------- 14/21 [openpyxl]\n",
      "   -------------------------- ------------- 14/21 [openpyxl]\n",
      "   -------------------------- ------------- 14/21 [openpyxl]\n",
      "   -------------------------- ------------- 14/21 [openpyxl]\n",
      "   -------------------------- ------------- 14/21 [openpyxl]\n",
      "   ---------------------------- ----------- 15/21 [opencv-python-headless]\n",
      "   ---------------------------- ----------- 15/21 [opencv-python-headless]\n",
      "   ---------------------------- ----------- 15/21 [opencv-python-headless]\n",
      "   ---------------------------- ----------- 15/21 [opencv-python-headless]\n",
      "   ------------------------------ --------- 16/21 [cffi]\n",
      "   ------------------------------ --------- 16/21 [cffi]\n",
      "   -------------------------------- ------- 17/21 [tabula-py]\n",
      "   ---------------------------------- ----- 18/21 [cryptography]\n",
      "   ---------------------------------- ----- 18/21 [cryptography]\n",
      "   ---------------------------------- ----- 18/21 [cryptography]\n",
      "   ---------------------------------- ----- 18/21 [cryptography]\n",
      "   ---------------------------------- ----- 18/21 [cryptography]\n",
      "   ---------------------------------- ----- 18/21 [cryptography]\n",
      "   ---------------------------------- ----- 18/21 [cryptography]\n",
      "   ---------------------------------- ----- 18/21 [cryptography]\n",
      "   ------------------------------------ --- 19/21 [pdfminer-six]\n",
      "   ------------------------------------ --- 19/21 [pdfminer-six]\n",
      "   ------------------------------------ --- 19/21 [pdfminer-six]\n",
      "   ------------------------------------ --- 19/21 [pdfminer-six]\n",
      "   ------------------------------------ --- 19/21 [pdfminer-six]\n",
      "   -------------------------------------- - 20/21 [camelot-py]\n",
      "   -------------------------------------- - 20/21 [camelot-py]\n",
      "   -------------------------------------- - 20/21 [camelot-py]\n",
      "   ---------------------------------------- 21/21 [camelot-py]\n",
      "\n",
      "Successfully installed PyPDF2-3.0.1 camelot-py-1.0.0 cffi-1.17.1 chardet-5.2.0 charset-normalizer-3.4.2 click-8.2.0 cryptography-44.0.3 distro-1.9.0 et-xmlfile-2.0.0 numpy-2.2.5 opencv-python-headless-4.11.0.86 openpyxl-3.1.5 pandas-2.2.3 pdfminer-six-20250506 pycparser-2.22 pypdf-5.5.0 pypdfium2-4.30.1 pytz-2025.2 tabula-py-2.10.0 tabulate-0.9.0 tzdata-2025.2\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install PyPDF2 tabula-py pandas camelot-py opencv-python-headless"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9929439a",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "Import the necessary libraries for PDF text and table extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56a38ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import PyPDF2\n",
    "import tabula\n",
    "import pandas as pd\n",
    "import camelot\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8eafb30",
   "metadata": {},
   "source": [
    "## Load PDF File\n",
    "\n",
    "Prompt the user to provide the path to the PDF file and load it for processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3a723e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF file loaded successfully: chapter4.pdf\n"
     ]
    }
   ],
   "source": [
    "# Function to check if a file exists and is a PDF\n",
    "def validate_pdf_path(pdf_path):\n",
    "    if not os.path.exists(pdf_path):\n",
    "        return False, \"File does not exist.\"\n",
    "    \n",
    "    if not pdf_path.lower().endswith('.pdf'):\n",
    "        return False, \"File is not a PDF.\"\n",
    "    \n",
    "    return True, \"PDF file is valid.\"\n",
    "\n",
    "# Get PDF path from user\n",
    "pdf_path = input(\"Enter the path to your PDF file: \")\n",
    "\n",
    "# Validate the PDF path\n",
    "is_valid, message = validate_pdf_path(pdf_path)\n",
    "\n",
    "if is_valid:\n",
    "    print(f\"PDF file loaded successfully: {pdf_path}\")\n",
    "else:\n",
    "    print(f\"Error: {message}\")\n",
    "    pdf_path = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be7a84f",
   "metadata": {},
   "source": [
    "## Extract Text from PDF\n",
    "\n",
    "Use PyPDF2 to extract text content from the PDF file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9569b08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF has 47 pages.\n",
      "\n",
      "Sample of extracted text (first page):\n",
      "--------------------------------------------------\n",
      "Preprocessing data\n",
      "SUPERVISED LEARNING WITH SCIKIT-LEARN\n",
      "George Boorman\n",
      "Core Curriculum Manager, DataCamp\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    if not pdf_path:\n",
    "        return None\n",
    "    \n",
    "    text_content = []\n",
    "    \n",
    "    try:\n",
    "        # Open the PDF file\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            # Create a PDF reader object\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            \n",
    "            # Get the number of pages in the PDF\n",
    "            num_pages = len(pdf_reader.pages)\n",
    "            \n",
    "            print(f\"PDF has {num_pages} pages.\")\n",
    "            \n",
    "            # Extract text from each page\n",
    "            for page_num in range(num_pages):\n",
    "                page = pdf_reader.pages[page_num]\n",
    "                page_text = page.extract_text()\n",
    "                text_content.append(page_text)\n",
    "                \n",
    "        return text_content\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Extract text from the PDF\n",
    "if pdf_path:\n",
    "    extracted_text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # Display a sample of the extracted text\n",
    "    if extracted_text:\n",
    "        print(\"\\nSample of extracted text (first page):\")\n",
    "        print(\"-\" * 50)\n",
    "        print(extracted_text[0][:500] + \"...\" if len(extracted_text[0]) > 500 else extracted_text[0])\n",
    "        print(\"-\" * 50)\n",
    "    else:\n",
    "        print(\"No text could be extracted from the PDF.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1680e0b2",
   "metadata": {},
   "source": [
    "## Extract Table Information\n",
    "\n",
    "Use both tabula-py and camelot to extract table data from the PDF. We'll try both libraries since table extraction can vary in accuracy depending on the PDF structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7cebd36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to import jpype dependencies. Fallback to subprocess.\n",
      "No module named 'jpype'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting tables from PDF...\n",
      "Extracted 3 tables with tabula-py.\n",
      "Error extracting tables with camelot: Image conversion failed with image conversion backend 'ghostscript'\n",
      " error: Ghostscript is not installed. You can install it using the instructions here: https://camelot-py.readthedocs.io/en/latest/user/install-deps.html\n",
      "\n",
      "Sample of first extracted table (tabula):\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>popularity</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>danceability</th>\n",
       "      <th>...</th>\n",
       "      <th>tempo</th>\n",
       "      <th>valence</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.6440</td>\n",
       "      <td>0.823</td>\n",
       "      <td>...</td>\n",
       "      <td>102.619000</td>\n",
       "      <td>0.649</td>\n",
       "      <td>Jazz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.0855</td>\n",
       "      <td>0.686</td>\n",
       "      <td>...</td>\n",
       "      <td>173.915000</td>\n",
       "      <td>0.636</td>\n",
       "      <td>Rap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.2390</td>\n",
       "      <td>0.669</td>\n",
       "      <td>...</td>\n",
       "      <td>145.061000</td>\n",
       "      <td>0.494</td>\n",
       "      <td>Electronic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>0.522</td>\n",
       "      <td>...</td>\n",
       "      <td>120.406497</td>\n",
       "      <td>0.595</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.1210</td>\n",
       "      <td>0.780</td>\n",
       "      <td>...</td>\n",
       "      <td>96.056000</td>\n",
       "      <td>0.312</td>\n",
       "      <td>Rap</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  popularity  acousticness  danceability  ...       tempo  \\\n",
       "0           0        41.0        0.6440         0.823  ...  102.619000   \n",
       "1           1        62.0        0.0855         0.686  ...  173.915000   \n",
       "2           2        42.0        0.2390         0.669  ...  145.061000   \n",
       "3           3        64.0        0.0125         0.522  ...  120.406497   \n",
       "4           4        60.0        0.1210         0.780  ...   96.056000   \n",
       "\n",
       "   valence       genre  \n",
       "0    0.649        Jazz  \n",
       "1    0.636         Rap  \n",
       "2    0.494  Electronic  \n",
       "3    0.595        Rock  \n",
       "4    0.312         Rap  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def extract_tables_with_tabula(pdf_path):\n",
    "    try:\n",
    "        # Extract tables using tabula\n",
    "        tables = tabula.read_pdf(pdf_path, pages='all', multiple_tables=True)\n",
    "        print(f\"Extracted {len(tables)} tables with tabula-py.\")\n",
    "        return tables\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting tables with tabula: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def extract_tables_with_camelot(pdf_path):\n",
    "    try:\n",
    "        # Extract tables using camelot\n",
    "        tables = camelot.read_pdf(pdf_path, pages='all')\n",
    "        print(f\"Extracted {len(tables)} tables with camelot.\")\n",
    "        return tables\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting tables with camelot: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "# Extract tables from the PDF using both methods\n",
    "tabula_tables = []\n",
    "camelot_tables = []\n",
    "\n",
    "if pdf_path:\n",
    "    print(\"\\nExtracting tables from PDF...\")\n",
    "    tabula_tables = extract_tables_with_tabula(pdf_path)\n",
    "    \n",
    "    try:\n",
    "        camelot_tables = extract_tables_with_camelot(pdf_path)\n",
    "    except:\n",
    "        print(\"Camelot extraction failed, continuing with tabula results only.\")\n",
    "    \n",
    "    # Display a sample of the first table if available\n",
    "    if tabula_tables and len(tabula_tables) > 0:\n",
    "        print(\"\\nSample of first extracted table (tabula):\")\n",
    "        print(\"-\" * 50)\n",
    "        display(tabula_tables[0].head())\n",
    "        print(\"-\" * 50)\n",
    "    elif camelot_tables and len(camelot_tables) > 0:\n",
    "        print(\"\\nSample of first extracted table (camelot):\")\n",
    "        print(\"-\" * 50)\n",
    "        display(camelot_tables[0].df.head())\n",
    "        print(\"-\" * 50)\n",
    "    else:\n",
    "        print(\"No tables were found in the PDF.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b53395c",
   "metadata": {},
   "source": [
    "## Process Extracted Data\n",
    "\n",
    "Process the extracted text to identify potential headers and content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "254e90bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Headers:\n",
      "--------------------------------------------------\n",
      "SUPERVISED LEARNING WITH SCIKIT-LEARN: Where to go from here? Machine Learning with Tree-...\n",
      "With real-world data: This is rarely the case We will often need to prep...\n",
      "Dealing with categorical features: scikit-learn will not accept categorical features ...\n",
      "Dummy variables: SUPERVISED LEARNING WITH SCIKIT-LEARNDummy variabl...\n",
      "Music dataset: print(music_df.isna().sum().sort_values()) genre  ...\n",
      "Encoding dummy variables: music_dummies = pd.get_dummies(music_df, drop_firs...\n",
      "X = music_dummies.drop(\"popularity\", axis=1).values: y = music_dummies[\"popularity\"].values X_train, X_...\n",
      "Handling missing: data\n",
      "Missing data: No value for a feature in a particular row\n",
      "This can occur because: There may have been no observation The data might ...\n",
      "Dropping missing data: music_df = music_df.dropna(subset=[\"genre\", \"popul...\n",
      "Imputing values: Imputation - use subject-matter expertise to repla...\n",
      "Imputation with scikit-learn: imp_num = SimpleImputer()\n",
      "X_cat = music_df[\"genre\"].values.reshape(-1, 1): X_num = music_df.drop([\"genre\", \"popularity\"], axi...\n",
      "X_test_num = imp_num.transform(X_test_num): X_train = np.append(X_train_num, X_train_cat, axis...\n",
      "Imputing within a pipeline: steps = [(\"imputation\", SimpleImputer()), (\"logist...\n",
      "X = music_df.drop(\"genre\", axis=1).values: y = music_df[\"genre\"].values X_train, X_test, y_tr...\n",
      "Centering and: scaling\n",
      "Why scale our data?: Many models use some form of distance to inform th...\n",
      "This is called standardization: Can also subtract the minimum and divide by the ra...\n",
      "Scaling in scikit-learn: from sklearn.preprocessing import StandardScaler\n",
      "X_test_scaled = scaler.transform(X_test): print(np.mean(X), np.std(X)) print(np.mean(X_train...\n",
      "Scaling in a pipeline: steps = [('scaler', StandardScaler()), ('knn', KNe...\n",
      "Checking model parameters: print(cv.best_score_) 0.8199999999999999 print(cv....\n",
      "Evaluating multiple: models\n",
      "Size of the dataset: Fewer features = simpler model, faster training ti...\n",
      "Interpretability: Some models are easier to explain, which can be im...\n",
      "Flexibility: May improve accuracy, by making fewer assumptions ...\n",
      "ROC AUC: Train several models and evaluate performance out ...\n",
      "KNN: Linear Regression (plus Ridge, Lasso)\n",
      "Artificial Neural Network: Best to scale our data before evaluating models\n",
      "Evaluating classification models: models = {\"Logistic Regression\": LogisticRegressio...\n",
      "X = music.drop(\"genre\", axis=1).values: y = music[\"genre\"].values X_train, X_test, y_train...\n",
      "for model in models.values(): kf = KFold(n_splits=6, random_state=42, shuffle=Tr...\n",
      "Test set performance: for name, model in models.items(): model.fit(X_tra...\n",
      "What you've covered: Using supervised learning techniques to build pred...\n",
      "Model Validation in Python: Feature Engineering for Machine Learning in Python\n",
      "extraction_date: 2025-05-17 19:08:22\n",
      "source_file: chapter4.pdf\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def process_text_for_headers(text_content):\n",
    "    headers = {}\n",
    "    \n",
    "    if not text_content or len(text_content) == 0:\n",
    "        return headers\n",
    "    \n",
    "    # Combine all text content\n",
    "    all_text = \"\\n\".join(text_content)\n",
    "    \n",
    "    # Split text into lines\n",
    "    lines = all_text.split('\\n')\n",
    "    \n",
    "    # Simple heuristic: Consider lines with fewer than 5 words and ending with a colon as potential headers\n",
    "    # This is a basic approach and may need refinement based on the actual PDF structure\n",
    "    current_header = None\n",
    "    current_content = []\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "            \n",
    "        words = line.split()\n",
    "        \n",
    "        # Check if this might be a header\n",
    "        if len(words) < 5 and (line.endswith(':') or line.isupper() or all(c.isupper() for c in line[0])):\n",
    "            # Save previous header and content\n",
    "            if current_header and current_content:\n",
    "                headers[current_header] = ' '.join(current_content)\n",
    "            \n",
    "            # Set new header\n",
    "            current_header = line.rstrip(':').strip()\n",
    "            current_content = []\n",
    "        else:\n",
    "            # Add to current content\n",
    "            if current_header:\n",
    "                current_content.append(line)\n",
    "            elif not headers.get('Title'):\n",
    "                # Use the first significant line as the title if no title exists yet\n",
    "                if len(line) > 5:\n",
    "                    headers['Title'] = line\n",
    "    \n",
    "    # Save the last header and content\n",
    "    if current_header and current_content:\n",
    "        headers[current_header] = ' '.join(current_content)\n",
    "    \n",
    "    # Add some metadata\n",
    "    headers['extraction_date'] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    headers['source_file'] = os.path.basename(pdf_path) if pdf_path else \"Unknown\"\n",
    "    \n",
    "    return headers\n",
    "\n",
    "# Process the extracted text to identify headers\n",
    "headers = {}\n",
    "if extracted_text:\n",
    "    headers = process_text_for_headers(extracted_text)\n",
    "    \n",
    "    print(\"\\nExtracted Headers:\")\n",
    "    print(\"-\" * 50)\n",
    "    for key, value in headers.items():\n",
    "        print(f\"{key}: {value[:50]}...\" if len(value) > 50 else f\"{key}: {value}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99b1dee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 18 list items from tables.\n",
      "Sample list items:\n",
      "--------------------------------------------------\n",
      "Item 1:\n",
      "{'Unnamed: 0': 0, 'popularity': 41.0, 'acousticness': 0.644, 'danceability': 0.823, '...': '...', 'tempo': 102.619, 'valence': 0.649, 'genre': 'Jazz'}\n",
      "Item 2:\n",
      "{'Unnamed: 0': 1, 'popularity': 62.0, 'acousticness': 0.0855, 'danceability': 0.686, '...': '...', 'tempo': 173.915, 'valence': 0.636, 'genre': 'Rap'}\n",
      "Item 3:\n",
      "{'Unnamed: 0': 2, 'popularity': 42.0, 'acousticness': 0.239, 'danceability': 0.669, '...': '...', 'tempo': 145.061, 'valence': 0.494, 'genre': 'Electronic'}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def process_tables_to_list_items(tabula_tables, camelot_tables):\n",
    "    list_items = []\n",
    "    \n",
    "    # Process tabula tables\n",
    "    for i, table in enumerate(tabula_tables):\n",
    "        table_dict = table.to_dict(orient='records')\n",
    "        for item in table_dict:\n",
    "            # Remove any items with all None or empty values\n",
    "            if any(v for v in item.values() if v is not None and str(v).strip()):\n",
    "                list_items.append(item)\n",
    "    \n",
    "    # Process camelot tables if tabula didn't find any\n",
    "    if not list_items and camelot_tables:\n",
    "        for i, table in enumerate(camelot_tables):\n",
    "            df = table.df\n",
    "            # If the first row contains headers, use it\n",
    "            if not df.empty:\n",
    "                headers = df.iloc[0].tolist()\n",
    "                for _, row in df.iloc[1:].iterrows():\n",
    "                    item = {}\n",
    "                    for j, header in enumerate(headers):\n",
    "                        if j < len(row):\n",
    "                            item[header] = row[j]\n",
    "                    # Remove any items with all None or empty values\n",
    "                    if any(v for v in item.values() if v is not None and str(v).strip()):\n",
    "                        list_items.append(item)\n",
    "    \n",
    "    return list_items\n",
    "\n",
    "# Process tables into list items\n",
    "list_items = []\n",
    "if tabula_tables or camelot_tables:\n",
    "    list_items = process_tables_to_list_items(tabula_tables, camelot_tables)\n",
    "    \n",
    "    print(f\"\\nProcessed {len(list_items)} list items from tables.\")\n",
    "    if list_items:\n",
    "        print(\"Sample list items:\")\n",
    "        print(\"-\" * 50)\n",
    "        for i, item in enumerate(list_items[:3]):\n",
    "            print(f\"Item {i+1}:\")\n",
    "            print(item)\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe222c5e",
   "metadata": {},
   "source": [
    "## Format Data into JSON\n",
    "\n",
    "Combine the extracted headers and list items into a structured JSON format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f86d0e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "JSON Data:\n",
      "--------------------------------------------------\n",
      "{\n",
      "  \"SUPERVISED LEARNING WITH SCIKIT-LEARN\": \"Where to go from here? Machine Learning with Tree-Based Models in Python Preprocessing for Machine Learning in Python\",\n",
      "  \"With real-world data\": \"This is rarely the case We will often need to preprocess our data first\",\n",
      "  \"Dealing with categorical features\": \"scikit-learn will not accept categorical features by default Need to convert categorical features into numeric values Convert to binary features called dummy variables 0: Observation was NOT that category 1: Observation was that category SUPERVISED LEARNING WITH SCIKIT-LEARNDummy variables\",\n",
      "  \"Dummy variables\": \"SUPERVISED LEARNING WITH SCIKIT-LEARNDummy variables\",\n",
      "  \"Music dataset\": \"print(music_df.isna().sum().sort_values()) genre                 8 popularity           31 loudness             44 liveness             46 tempo                46 speechiness          59 duration_ms          91 instrumentalness     91 danceability        143 valence             143 acousticness        ...\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def format_data_to_json(headers, list_items):\n",
    "    json_data = {}\n",
    "    \n",
    "    # Add headers to the JSON\n",
    "    for key, value in headers.items():\n",
    "        json_data[key] = value\n",
    "    \n",
    "    # Add list items\n",
    "    json_data[\"List_items\"] = list_items\n",
    "    \n",
    "    return json_data\n",
    "\n",
    "# Format the data into JSON\n",
    "json_data = format_data_to_json(headers, list_items)\n",
    "\n",
    "# Display the JSON data\n",
    "print(\"\\nJSON Data:\")\n",
    "print(\"-\" * 50)\n",
    "print(json.dumps(json_data, indent=2, ensure_ascii=False)[:1000] + \"...\" if len(json.dumps(json_data, indent=2)) > 1000 else json.dumps(json_data, indent=2))\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ddd6ba",
   "metadata": {},
   "source": [
    "## Save JSON to File\n",
    "\n",
    "Save the formatted JSON data to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ea3edf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON data saved to chapter4_extracted.json\n",
      "\n",
      "Extraction complete! Data saved to chapter4_extracted.json\n"
     ]
    }
   ],
   "source": [
    "def save_json_to_file(json_data, pdf_path):\n",
    "    if not pdf_path:\n",
    "        output_path = \"extracted_data.json\"\n",
    "    else:\n",
    "        # Use the PDF filename as the base for the JSON file\n",
    "        base_name = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "        output_path = f\"{base_name}_extracted.json\"\n",
    "    \n",
    "    try:\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(json_data, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"JSON data saved to {output_path}\")\n",
    "        return output_path\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving JSON data: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Save the JSON data to a file\n",
    "if json_data:\n",
    "    output_path = save_json_to_file(json_data, pdf_path)\n",
    "    if output_path:\n",
    "        print(f\"\\nExtraction complete! Data saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb1d4d9",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook provides a comprehensive solution for extracting text and table data from PDF documents and formatting it into a structured JSON file. The solution:\n",
    "\n",
    "1. Extracts text using PyPDF2\n",
    "2. Extracts tables using both tabula-py and camelot for better coverage\n",
    "3. Processes text to identify potential headers\n",
    "4. Formats table data into list items\n",
    "5. Combines all extracted data into a structured JSON format\n",
    "6. Saves the JSON data to a file\n",
    "\n",
    "You can modify the processing logic to better suit specific PDF structures if needed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
